{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d56324b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pdf2image\n",
    "import pytesseract\n",
    "import nltk\n",
    "import hashlib\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb9e289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Briefgarde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "379198cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDoc(url, pagenumber):\n",
    "    pdf = requests.get(url) \n",
    "    # tissue scale\n",
    "    doc = pdf2image.convert_from_bytes(pdf.content)\n",
    "    # Get the article text\n",
    "    article = []\n",
    "    for page_number, page_data in enumerate(doc):\n",
    "        txt = pytesseract.image_to_string(page_data).encode(\"utf-8\")\n",
    "        # Sixth page are only references\n",
    "        if page_number < pagenumber:\n",
    "            article.append(txt.decode(\"utf-8\"))\n",
    "    article_txt = \" \".join(article)\n",
    "    return article_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "464c805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, threshold):\n",
    "    ctext = text.split(threshold)[1]\n",
    "    \"\"\"Remove section titles and figure descriptions from text\"\"\"\n",
    "    clean = \"\\n\".join([row for row in ctext.split(\"\\n\") if\n",
    "                      (len(row.split(\" \"))) > 3 and not (row.startswith(\"(a)\"))\n",
    "                      and not row.startswith(\"Figure\")])\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98bc9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homemadeTokenizer(doc):\n",
    "    return nltk.tokenize.sent_tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d06a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_plain2(text, url=\"http://bern2.korea.ac.kr/plain\"):\n",
    "    \"\"\"Biomedical entity linking API\"\"\"\n",
    "    try:\n",
    "        response = requests.post(url, json={'text': str(text)})\n",
    "        response.raise_for_status()  # Raise an error for bad responses (4xx and 5xx)\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97223ec8",
   "metadata": {
    "tags": [],
    "timeout": null
   },
   "outputs": [],
   "source": [
    "def getData(sentences):\n",
    "    entity_list = []\n",
    "    print(len(sentences))\n",
    "\n",
    "    i=0\n",
    "    # The last sentence is invalid\n",
    "    for s in sentences[:-1]:\n",
    "        print(\"doing sentence : \" + str(i))\n",
    "        entity_list.append(query_plain2(s))\n",
    "        i += 1\n",
    "\n",
    "    print(\"all calls done\")\n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "768f7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(entity_list):\n",
    "    parsed_entities = []\n",
    "\n",
    "    filtered_entity_list = [entities for entities in entity_list if entities is not None]\n",
    "\n",
    "    for entities in filtered_entity_list:\n",
    "        e = []\n",
    "\n",
    "        # If there are no entities in the text\n",
    "        if not entities.get('annotations'):\n",
    "            parsed_entities.append({\n",
    "                'text': entities['text'],\n",
    "                'text_sha256': hashlib.sha256(entities['text'].encode('utf-8')).hexdigest()\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        for entity in entities['annotations']:\n",
    "            other_ids = [id for id in entity['id'] if not id.startswith(\"BERN\")]\n",
    "            entity_type = entity['obj']\n",
    "            entity_name = entities['text'][entity['span']['begin']:entity['span']['end']]\n",
    "\n",
    "            try:\n",
    "                entity_id = [id for id in entity['id'] if id.startswith(\"BERN\")][0]\n",
    "            except IndexError:\n",
    "                entity_id = entity_name\n",
    "\n",
    "            e.append({\n",
    "                'entity_id': entity_id,\n",
    "                'other_ids': other_ids,\n",
    "                'entity_type': entity_type,\n",
    "                'entity': entity_name\n",
    "            })\n",
    "\n",
    "        parsed_entities.append({\n",
    "            'entities': e,\n",
    "            'text': entities['text'],\n",
    "            'text_sha256': hashlib.sha256(entities['text'].encode('utf-8')).hexdigest()\n",
    "        })\n",
    "\n",
    "    return parsed_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea0a69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'bolt://52.91.239.25:7687'\n",
    "user = 'neo4j'\n",
    "password = 'house-round-forearm'\n",
    "driver = GraphDatabase.driver(host,auth=(user, password))\n",
    "def neo4j_query(query, params=None):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, params)\n",
    "        return pd.DataFrame([r.values() for r in result], \n",
    "columns=result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bb884e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParsedEntity(urlToPDF, numInterestingPage, threshold):\n",
    "    article_txt = getDoc(urlToPDF, numInterestingPage)\n",
    "    print(\"GOt article text\")\n",
    "    ctext = clean_text(article_txt, threshold)\n",
    "    print(\"get clean text\")\n",
    "    sentences = homemadeTokenizer(ctext)\n",
    "    print(\"got Sentences : \")\n",
    "    print(sentences)\n",
    "    entity_list = []\n",
    "    entity_list = getData(sentences)\n",
    "    print(\"Got obtained data from BERN2\")\n",
    "    parsed_entities = parseData(entity_list)\n",
    "    print(\"Parsed Everything\")\n",
    "    return parsed_entities\n",
    "    # By this point, we have the data and everything. We now need to push things to the Graph. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dbd7c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushtoGraph(parsedEntity, authors, title):\n",
    "    neo4j_query(\"\"\"\n",
    "    MERGE (b:Article {title: $title})\n",
    "    WITH b\n",
    "    UNWIND $authors AS authorName\n",
    "    MERGE (a:Author {name: authorName})\n",
    "    MERGE (a)-[:WROTE]->(b)\n",
    "    \"\"\", {'title': title, 'authors': authors})\n",
    "    print(\"Created the base article and linked the authors\")\n",
    "    neo4j_query(\"\"\"\n",
    "    MATCH (a:Article {title: $title})\n",
    "    WITH a\n",
    "    OPTIONAL MATCH (a)-[:HAS_SENTENCE]->(s:Sentence)\n",
    "    WHERE s IS NULL\n",
    "    WITH a\n",
    "    UNWIND $data as row\n",
    "    MERGE (s:Sentence {id: row.text_sha256})\n",
    "    SET s.text = row.text\n",
    "    MERGE (a)-[:HAS_SENTENCE]->(s)\n",
    "    WITH s, row.entities as entities\n",
    "    UNWIND entities as entity\n",
    "    MERGE (e:Entity {id: entity.entity_id})\n",
    "    ON CREATE SET e.other_ids = entity.other_ids,\n",
    "                  e.name = entity.entity,\n",
    "                  e.type = entity.entity_type\n",
    "    MERGE (s)-[m:MENTIONS]->(e)\n",
    "    ON CREATE SET m.count = 1\n",
    "    ON MATCH SET m.count = m.count + 1\n",
    "    \"\"\", {'data': parsedEntity, 'title': title})\n",
    "    print(\"Pushed all data to graph !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca833c",
   "metadata": {},
   "source": [
    "For some reasons, the cell below doesn't work unless I put the imports in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80649963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "token loaded\n"
     ]
    }
   ],
   "source": [
    "from zero_shot_re import RelTaggerModel, RelationExtractor\n",
    "from transformers import AutoTokenizer\n",
    "model = RelTaggerModel.from_pretrained(\"fractalego/fewrel-zero-shot\")\n",
    "print(\"model loaded\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "print(\"token loaded\")\n",
    "relations = ['associated', 'interacts']\n",
    "extractor = RelationExtractor(model, tokenizer, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfa567d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelations(parsed_entities):\n",
    "    present_candidates = [s for s in parsed_entities if (s.get('entities')) and (len(s['entities']) > 1)]\n",
    "    predicted_rels = []\n",
    "\n",
    "    for c in present_candidates:\n",
    "        combinations = itertools.combinations([{'name': x['entity'], 'id': x['entity_id']} for x in c['entities']], 2)\n",
    "\n",
    "        for combination in list(combinations):\n",
    "            try:\n",
    "                ranked_rels = extractor.rank(text=c['text'].replace(\",\", \"\"), head=combination[0]['name'], tail=combination[1]['name'])\n",
    "\n",
    "                # Define threshold for the most probable relation\n",
    "                if ranked_rels[0][1] > 0.85:\n",
    "                    predicted_rels.append({\n",
    "                        'head': combination[0]['id'],\n",
    "                        'tail': combination[1]['id'],\n",
    "                        'type': ranked_rels[0][0],\n",
    "                        'source': c['text_sha256']\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                # Handle exceptions appropriately\n",
    "                pass\n",
    "    print(\"got predicted rels\")\n",
    "    return predicted_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e5255d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushRelationToGraph(predictedRels):\n",
    "    neo4j_query(\"\"\"\n",
    "    UNWIND $data as row\n",
    "    MATCH (source:Entity {id: row.head})\n",
    "    MATCH (target:Entity {id: row.tail})\n",
    "    MATCH (text:Sentence {id: row.source})\n",
    "    MERGE (source)-[:REL]->(r:Relation {type: row.type})-[:REL]-\n",
    "    >(target)\n",
    "    MERGE (text)-[:MENTIONS]->(r)\n",
    "    \"\"\", {'data': predictedRels})\n",
    "    print(\"Pushed rels to graph\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d09c9d4",
   "metadata": {},
   "source": [
    "authors =[\"Patricia Rousselle\", \"Chloé Laigle\", \"Gaelle Rousselet\"]\n",
    "title = \"The dermal-epidermal junction instructs epidermal keratinocytes\"\n",
    "threshold = \"Introduction\"\n",
    "numInterestingPage = 28\n",
    "url = \"https://arxiv.org/ftp/arxiv/papers/2311/2311.15662.pdf\"\n",
    "parsedstuff = getParsedEntity(url, numInterestingPage, threshold)\n",
    "print(\"all stuff parsed\")\n",
    "pushtoGraph(parsedstuff, authors, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8702a723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOt article text\n",
      "get clean text\n",
      "got Sentences : \n",
      "['The advent of large language models (LLMs) has demon-\\nstrated substantial potential for diverse real-world applica-\\ntions, thanks to their remarkable language understanding ca-\\npabilities.', 'In the medical domain, a notable number of Chi-\\nnese medical LLMs have successively emerged, including\\nHuaTuo (Wang et al.', '2023), ChatMed (Zhu and Wang 2023),\\nBianQue (Chen et al.', '2023), Sunsimiao (Xin Yan 2023), and\\nDoctorGLM (Xiong et al.', '2023), to better assist doctors in\\ndiverse tasks ranging from clinical diagnosis to disease pre-\\nvention (Singhal et al.', '2023a).', 'This underscores an urgent\\nneed for a standardized medical benchmark, capable of of-\\nfering reliable and authoritative evaluations for such LLMs.', '2 (Chinese Medical Standardization —_ Qualification\\na Licensing Exam) — Training Exam Exam\\nse Medical Schoo!', 'Residency Doctor in-charge Real-world Case\\n3 NMPQE (Medical Licensing Exam)\\n.', 'Medical School Residency -\\ng USMLE USMLE USMLE\\nclinical knowledge _. generalist medical practice\\nAssessing the potential and inherent limitations of medi-\\ncal LLMs from diverse perspectives continues to present\\nconsiderable challenges (Singhal et al.', '2023a; Chang et al.', 'The primary cause of this issue lies in the pronounced dis-\\ncrepancy between the existing benchmarks and the practical\\nrealities of medicine, leading to an urgent need for advances\\nin evaluation standards.', 'Widely used medical benchmarks\\nsuch as MedQA (Jin et al.', '2021) and MedMCQA (Pal, Uma-\\npathi, and Sankarasubbu 2022) typically encompass pub-\\nlicly available medical question-answering datasets gathered\\nfrom textbooks, research papers, and board exams.', 'How-\\never, in light of recent research employing these open-access\\ndatasets for model training (Han et al.', '2023), a process po-\\ntentially resulting in data contamination with regard to the\\nevaluation (Nori et al.', '2023), it is increasingly clear that\\nthe prevailing benchmarks exhibit considerable limitations.', 'A recent study has made advances by introducing a hu-\\nman evaluation framework along with MultiMedQA (Sing-\\nhal et al.', '2023a), a multi-faceted benchmark that combines\\nexisting QA datasets with a novel set of online medical\\nquestions.', 'Nonetheless, given the disparities among typo-\\nlogically diverse languages, we are unable to rely on this\\nbenchmark for assessing Chinese large language models, in\\nparticular considering the unique clinical standards and pro-\\ncedures across different countries illustrated in Figure 1.', 'Hence, several Chinese benchmarks have been proposed,\\nincluding a medical NLP-task oriented one!', 'and others\\n Figure 2: Examples of prompts and corresponding answers.', 'The left side shows the prompt and responses of LLMs on an\\nexample question from an exam.', 'The right side is an example of a real-world case.', 'based on the Chinese National Medical Licensing Examina-\\ntion (CNMLE), including MLEC-QA (Li, Zhong, and Chen\\n2021) and CMExam (Liu et al.', '2023).', 'While these bench-\\nmarks provide valuable insights, they are not exhaustive and\\nmay fall short in enabling us to comprehensively gauge the\\npotential of LLMs with respect to all-round medical knowl-\\nedge and their practical utility in real-world clinical diagnos-\\ntic scenarios.', 'In mainland China, a unique three-stage ex-\\namination process is employed, key components of which,\\nincluding the Resident Standardization Training and Doc-\\ntor in-charge Qualification Exams, have largely been over-\\nlooked in prior work.', 'Furthermore, the evaluation of real-\\nworld clinical practical skills has not been adequately incor-\\nporated in prior work.', 'To address these gaps and align with the learning and\\ngrowth trajectory of Chinese doctors, we introduce Med-\\nBench, a novel large-scale Chinese medical benchmark,\\nwhich encompasses both authentic three-stage medical ex-\\naminations and real-world clinical diagnosis cases.', 'It sur-\\npasses prior benchmarks by being exclusively sourced from\\nthe latest validated exams and expert-annotated EHRs,\\nthereby ensuring compliance with medical standards and\\npractices.', 'Furthermore, we conduct extensive experiments\\nand offer detailed analyses to provide diverse perspectives\\nfor evaluating clinical knowledge recall and reasoning capa-\\nbilities of LLMs across a range of branches of medicine.', 'The\\nmain findings on this benchmark are as follows:\\n* Chinese medical large language models underperform on\\nthis benchmark, necessitating substantial improvements\\nin clinical knowledge and diagnostic accuracy, as well as\\nrefinement of their original in-context learning abilities.', '* Several large language models designed for general-\\ndomain tasks possess substantial medical knowledge,\\nthereby exhibiting promising potential.', '¢ Human evaluation reveals that ChatGPT possesses rich\\nknowledge for clinical practices, while current Chinese\\nmedical LLMs lack high-quality conversational abilities\\nand sufficient medical knowledge.', '* ChatGPT’s unsatisfactory performance in case analysis,\\na question type that demands profound medical knowl-\\nedge understanding and clinical reasoning abilities, re-\\nveals significant room for improvement in medicine.', 'In summary, this study presents a comprehensive benchmark\\nthat aligns with the practical realities of medicine in main-\\nland China and provides profound empirical findings to re-\\nveal the medical capabilities and limitations of LLMs.', 'Fur-\\nthermore, we incorporate Item Response Theory (Birnbaum\\n1969) to further enhance our benchmark, with the ultimate\\ngoal of aiding the medical research community.', 'Overview For better evaluating medical foundation mod-\\nels, this paper proposes MedBench as a large benchmark\\nwith 40,041 exercises originating from both authentic med-\\nical examinations and real-world diagnostic and treatment\\ncases.', 'Specifically, we collect three-stage medical examina-\\ntions that serve as a faithful reflection of the comprehensive\\nprocess involved in obtaining medical licenses in mainland\\nChina, exemplifying essential medical knowledge.', 'More-\\nover, we construct a number of real-world cases based on\\nelectronic health records that provide examination plans, di-\\nagnoses, and treatments based on patients’ symptoms, which\\ncan reveal the medical knowledge utilization and reasoning\\ncapabilities of LLMs in the real world.', 'Construction and Statistics Regarding the three-stage\\nmedical examinations, we collect representative exercises\\nfrom the Chinese Medical Licensing Exam (CNMLE), Res-\\nident Standardization Training Exam, and Doctor in-charge\\nQualification Exam?', 'across many recent years, covering\\n27,248, 2,841, and 8,927 questions, respectively.', 'The ex-\\namination exercises, depicted on the left of Figure 2, are\\nmultiple-choice questions that can be categorized into three\\n* Al/A2/B: Single statement questions with one correct\\nanswer out of five options.', 'Endocrinology _ Plastic Surgery\\nthe Doctor in-charge Qualification Exam.', '« A3/A4: A series of questions accompanied by a clinical\\ncase with one correct answer out of five options.', '* Case Analysis: Given a clinical case, a series of ques-\\ntions are created with 6-12 options per question.', 'Some\\nquestions may have more than one correct answer.', 'for some examinations in MedBench.', 'Note that we use in-\\nternal medicine and surgery as representative examples for\\nthe latter two stages, considering the vast number of subcat-\\negories within these fields.', 'Furthermore, we collect over 2,000 real-world electronic\\nhealth records, and employ experts to identify the symptoms,\\ndiagnoses, treatments, and examinations from the reports,\\nresulting in 701 high-quality ones with an average length\\nof 60-100 words per report (Figure 2).', 'The annotators com-\\npleted professional training and strictly following predefined\\nannotation standards to ensure accuracy.', 'Based on these an-\\nnotated reports, a total of 1,025 question—answer pairs are\\nCharacteristics MedBench surpasses existing bench-\\nmarks in several aspects: (1) Authenticity and Novelty.', 'It\\nexclusively leverages expert-annotated EHRs and authentic\\nup-to-date medical examinations to mitigate contamination.', '(2) Comprehensiveness and Multi-facetedness.', 'It is metic-\\nulously designed to align with Chinese medical standards\\nand practices by incorporating three-stage multi-disciplinary\\nexaminations and real-world clinical cases.', '(3) Practicality.', 'Human evaluation on clinical real-world cases ensures con-\\ngruence with practical realities of medicine, while difficulty-\\nstratified divisions in MedBench enable rapid assessment.', 'Models and Evaluation Metrics To evaluate the medi-\\ncal capabilities, we conduct assessments using MedBench\\nwith several representative LLMs from both the general\\nand medical domains, including ChatGPT, ChatGLM (Zeng\\net al.', '2023; Du et al.', '2022), Baichuan-13B*, HuaTuo, and\\nChatMed.', 'Furthermore, we evaluate other Chinese medical\\nLLMs, such as BianQue, but we have observed that it lacks\\nthe ability to deliver accurate and reasonable responses to\\nFor the three-stage multiple-choice examinations, we em-\\nploy accuracy as the evaluation metric.', 'When dealing with\\nreal-world cases, we combine expert-level human evaluation\\nwith the additional automatic evaluation metrics BLEU (Pa-\\npineni et al.', '2002) and ROUGE (Lin 2004).', 'Experimental Settings We conduct extensive experi-\\nments to evaluate the five-shot performance of LLMs, ensur-\\ning their capability to respond in a multiple-choice format.', 'We leverage the API for ChatGPT* and opt for local deploy-\\nment to facilitate evaluations for other LLMs.', 'Furthermore,\\nwe partition the MedBench dataset based on exams, medical\\nsubdiscipline, and question types, and perform independent\\ntesting on each subset to enable a comprehensive analysis.', 'Three-stage examination results.', 'In Table 1, we present\\na comprehensive analysis of the accuracy metrics for vari-\\nous LLMs across the three exams.', 'A salient observation is\\nthat ChatGPT consistently surpasses other models, despite\\nthe latter being training on extensive Chinese corpora or pre-\\nmium medical datasets.', 'Nevertheless, our observations indi-\\ncate that ChatGPT’s accuracy rate hovers around 50% for\\nCNMLE and approximately 60% for other assessments, ex-\\nposing substantial avenues for enhancement.', 'As depicted in Figure 5, ChatGPT exhibits subpar per-\\nformance on questions pertaining to Traditional Chinese\\nMedicine (TCM) and Chinese Western Medicine (CWM),\\nwith accuracy metrics oscillating between 40-45%.', 'Con-\\nversely, LLMs trained on a more expansive Chinese dataset\\ndemonstrate a narrower performance disparity on TCM and\\nCWM questions relative to ChatGPT.', 'This suggests that\\na contributing factor to ChatGPT’s diminished efficacy on\\nMedBench may be attributed to its limited exposure to Chi-\\n LLM CNMLE Resident Standardization Training Doctor in-Charge Qualification\\nTotal A1/A2/B_ A3/A4 | Total A1I/A2/B_ A3/A4_—_ Cases Analysis | Total A1/A2/B_ A3/A4_— Cases Analysis\\nGPT-4 64.88 63.08 69.03 | 75.64 77.08 75.13 75.00 68.45 71.91 68.24 62.80\\nChatGPT 49.57 49.40 51.85 | 60.59 61.30 58.72 62.96 58.75 58.04 59.73 65.52\\nChatGLM 27.39 27.32 28.16 | 29.96 28.41 33.59 29.63 27.52 26.06 31.43 28.97\\nBaichuan-13B | 30.47 30.54 29.63 | 34.97 37.26 32.70 22.65 29.56 31.31 31.65 17.89\\nHuaTuo 22.31 22.38 21.47 | 23.32 23.53 23.85 13.58 21.93 22.14 21.85 18.62\\nChatMed 23.45 23.46 23.33 | 24.33 23.03 26.54 32.10 24.02 23.46 24.66 30.34\\nTable 1: Results on three-stage medical examinations of MedBench.', 'nese data during its pretraining phase, consequently affect-\\ning its proficiency in Chinese medical knowledge.', 'It is imperative to underscore that Baichuan-13B and Hua-\\nTuo encounter difficulties on case analysis questions, high-\\nlighting potential domains for refinement in their logical rea-\\nsoning or multi-turn dialogue competencies.', 'We also perform comparative evaluations with a number\\nof recently-developed models, including MedLLaMA (Wu\\net al.', '2023), Baize (Xu et al.', '2023a), and ChatDoctor (Li\\net al.', '2023).', 'However, we have not noticed any signifi-\\ncant enhancements in performance on our benchmark com-\\npared to these models.', 'For example, MedLLaMA achieves\\nan accuracy of 24.5% on the Al/A2/B type of questions,\\nmarginally outperforming ChatMed with a modest improve-\\nReal-world clinical case performance.', 'Table 2 provides\\nthe results of the assessment on real-world cases, and Fig-\\nure 2 provides the prompt-response instances.', 'We evalu-\\nated the outcomes using the BLEU and ROUGE F1-score\\nmetrics.', 'It can be easily observed that ChatGPT and GPT-\\n4 (OpenAI 2023) exhibit superior performance, underscor-\\ning their remarkable aptitude in medical and conversational\\nquestion answering.', 'However, it is notable that even for\\nChatGPT and GPT-4, the top-performers across multiple tri-\\nals, the scores remain relatively moderate.', 'This could be at-\\ntributed, in part, to the fact that metrics such as BLEU and\\nROUGE might not holistically capture result quality.', 'Fur-\\nthermore, considerable scope remains for enhancing these\\nLLMs’ capabilities in real-world clinical cases.', 'Beyond automated evaluations, the judgment of a post-\\ngraduate medical scholar was solicited to appraise the out-\\nputs delivered by the various LLMs on the real-world cases.', 'The objective of this evaluation endeavor was to quanti-\\ntatively assess the correctness, completeness, fluency, and\\nfriendliness of HuaTuo, ChatMed, ChatGPT, and GPT-\\n4.', 'Figure 4 provides the outcomes of this human eval-\\nuation.', 'GPT-4 is found to consistently manifest superior\\nperformance across all delineated criteria, while ChatGPT\\nmarginally trails GPT-4, particularly on correctness and\\ncompleteness.', 'Conversely, HuaTuo’s rating in friendliness\\nis somewhat diminished, predominantly attributable to spo-\\nradic generation of incongruous content.', 'However, it is im-\\nportant to note that HuaTuo demonstrates pronounced levels\\nof correctness and completeness, evincing profound medical\\nknowledge, though its articulation warrants improvement.', 'ChatMed, while laudable for its fluency and friendliness,\\nregisters suboptimal scores in correctness and completeness.', 'Such disparities might be indicative of a potential attenua-\\ntion of medical expertise during the fine-tuning phase.', 'Comparison on different question types.', 'Figure 5 pro-\\nvides the detailed results on CNMLE, the Resident Stan-\\ndardization Training Exam (denoted as “Resident’”) and the\\nDoctor in-charge Qualification Exam (“In-charge”).', 'Clearly,\\nChatGPT continues to exhibit a substantial performance ad-\\nvantage over the other LLMs.', 'A3/A4 questions and case\\nanalysis questions, characterized by their multi-question\\nstructure, present a substantial challenge to the conversa-\\ntional abilities of the models.', 'If a given LLM fares poorly\\non these question types, it indicates its inherent limitations\\nin efficiently managing multi-turn dialogue or questions ne-\\ncessitating intricate reasoning steps.', 'LLM Vanilla prompt Chain-of-Thought\\nTable 3: A comparative assessment of Baichuan-13B and\\nChatGLM-6B using Al/A2 questions from the Resident\\nStandardization Training Exam under different prompts.', 'Chain-of-Thought In the experimental investigation, it is\\nobserved that prompts tailored to the LLM have the poten-\\ntial to enhance the reasoning capabilities of the LLM.', 'Fig-\\nure 7 illustrates the choice distribution of Baichuan-13B un-\\nder both the vanilla prompt and Chain-of-Thought prompt-\\ning.', 'Notably, under the vanilla prompt, Baichuan-13B shows\\na strong inclination towards option F, although this option is\\nnot part of the valid choice set (A-E).', 'In contrast, when us-\\ning Chain-of-Thought prompting, Baichuan-13B primarily\\ngravitates towards valid choices.', 'Furthermore, Table 3 de-\\ntails the accuracy associated with both the vanilla prompt\\nand the Chain-of-Thought one.', 'The data suggests a signif-\\nicant improvement in the accuracy of Baichuan-13B when\\nusing Chain-of-Thought prompting.', 'Parallel experiments\\nare conducted on ChatGLM-6B, and the improvement in ac-\\ncuracy when using Chain-of-Thought prompting is negligi-\\nFewer questions, higher differentiation During the as-\\nsessment process, some shortcomings became apparent:\\n(1) Certain LLMs manifest suboptimal inference speed on\\nGPUs or high computational cost when interfacing via\\nT D E T D E T D\\n11.259 12.17 3.09 166 2.36 0.95 0.82 2.66\\n8.37 7.29 3.26 1.82 1.85 15.39 11.30 10.04\\n212 1.90 0.11 0.22 0.22 1.79 3.62 3.29\\n5.29 10.14 1.50 0.52 2.59 0.71 0.0 1.11\\n3.77 2.32 0.23 042 0.15 243 0.52 1.06\\n488 5.79 0.62 0.84 0.46 1.35 0.55 0.26\\nTable 2: Evaluation on cases (E stands for Examinations, T stands for Treatments,\\ncases.', 'and D stands for Diagnoses)\\nan API.', 'Given that MedBench comprises approximately\\n40,000 questions, performing inference on the entirety of\\nthis dataset consecutively takes considerable time for LLMs.', '(2) While categorization of questions based on the type pro-\\nvides a rudimentary gauge of difficulty, it is important to ac-\\nknowledge that questions within the same category can ex-\\nhibit disparate levels of difficulty.', '(3) Assigning particularly\\nchallenging questions to less capable LLMs is inadvisable,\\nas it may culminate in uniformly diminished accuracy, mak-\\ning meaningful distinctions impossible.', 'To ameliorate these challenges, we propose methodolog-\\nical strategies to classify questions of analogous types ac-\\ncording to their inherent difficulty gradients.', 'By adopting\\nthis paradigm, we can optimize the evaluation process, al-\\nlowing LLMs to undertake inference on a curtailed set of\\nquestions.', 'As a result, this allows for a more nuanced align-\\nment between LLMs and questions of commensurate dif-\\nficulty tiers, congruent with their individual proficiencies.', 'Specifically, we introduce an advanced evaluative frame-\\nwork for LLMs drawing inspiration from Item Response\\nTheory (IRT).', 'Our approach integrates the three-parameter\\nlogistic model (IRT-3PL), given by:\\nP(Xij = 119;) TpeaGay\\nHere, 0; represents the proficiency of LLM j, and P(X; =\\n|0;) is the probability that an LLM j with proficiency 0;\\ngives a correct response to question i.', 'This equation high-\\nlights three crucial parameters for each question 7: discrim-\\nination (a;), difficulty (b;), and the guessing factor (c;).', 'In\\nour approach, we assume constant values for both a; and\\nc;, and then group items based on the shared difficulty met-\\nric b;.', 'For a specific difficulty subset, these three parameters\\nremain unchanged.', 'The equation captures the LLM’s prob-\\nability 6;, determined from the inference results when inter-\\nacting with a specific subset.', 'A total of 7,335 questions were\\ndivided into 10 difficulty levels.', 'To validate question differ-\\nentiation, we analyze them using BLOOMZ-7.1B (Muen-\\nnighoff et al.', '2022), Qwen-7B>, ChatGLM-turbo (Zeng et al.', '2023), Qwen-max (Bai et al.', '2023) across various difficulty\\nlevels.', 'Part (a) of Figure 8 displays the accuracy trends of\\nthese LLMs across these levels, highlighting a decrease in\\naccuracy with increasing difficulty.', 'Figure 8(b) shows the\\ndifference in accuracy between BLOOMZ-7.1B and Qwen-\\n7B, as well as between Qwen-max and ChatGLM-turbo, for\\ndifferent levels of question difficulty.', 'It is noticeable that the\\ndifferences between the LLMs are significant when the ques-\\ntion difficulty is appropriate.', 'In MedBench, there are several types of reasoning:\\n¢ Multi-condition single-hop reasoning: A type of rea-\\nsoning that requires the LLM to engage in single-hop\\nreasoning based on a clinical scenario.', '¢ Statement identification: A type of reasoning that re-\\nquires the LLM to judge the correctness of multiple state-\\nments, which further tests the LLM’s knowledge base.', '¢ Multi-hop reasoning: A type of reasoning with multiple\\nquestions, where the questions are related, requiring the\\nLLM to perform multi-hop reasoning.', 'To ascertain the extent to which LLMs make accurate deci-\\nsions based on their grasp of pertinent knowledge, we ex-\\ntended our inquiry, which entailed requesting the LLMs to\\nfurnish justifications for their responses, as depicted in Fig-\\nure 6.', 'Our observations reveal that LLMs can substantiate\\ntheir answers when they are accurate.', 'Conversely, in cases\\nwhere erroneous responses are given, the accompanying ex-\\nplanations often prove illogical, which provides compelling\\nevidence that these LLMs either lack the requisite knowl-\\nedge in the domain or are incapable of rationalizing towards\\nTraditionally, medical LLMs relied on classic medical QA\\nbenchmarks for evaluation.', 'Some studies used USMLE for\\nevaluation and achieved satisfactory results (Kung et al.', '2023; Nori et al.', '2023), with zero-shot GPT-4 achieving av-\\nerage scores of 86.65% and 86.7% on the Self-Assessment\\nand Sample Exam parts of USMLE, respectively.', 'However,\\nquestions from USMLE have a very distinct Western med-\\nical perspective that is notably different from that of Chi-\\nnese medicine.', 'As two distinct medical systems, the latter\\nincludes traditional Chinese medicine, with obvious differ-\\nences in concepts, diagnosis, and treatment methods.', 'There-\\nfore, the same symptoms may lead to different diagnoses\\nand treatments.', 'As a result, the USMLE cannot serve as an\\nadequate benchmark for Chinese medical practice.', 'MedQA,\\nPubMedQA (Jin et al.', '2019; Liévin, Hother, and Winther\\n CNMLE Clinic CNMLE Stomatology CNMLE Public Health CNMLE Traditional Chinese Medicine CNMLE Chinese Western Medicine\\nlm totel Mm total || = im total ME total ME tote!', 'M/AVB | 1/42/B marie | © AV/A2/B | AV/AD/B\\nRM wim | @ 13/4 A3/a4 13/A4\\neo se 8 ae eB aw\\n£ SF FS S¥ss ££ S FS FS Fs ey SS FS SF ey SF SF SF SF\\nOSS OFS a SPP KK FSS PF SK ae vs\\nCardiology of Resident Respiratory of Resident Gastroenterology of Resident Hematology of Resident\\ntotal total ° 10 total\\nM/A N/R » AV/A2\\n“ & ss 2s s ey id ea » ro ss >» 3 ey s s es\\n$ ” a sf é Ss s ES & * ¢ s *\\n¥ ¥ * *\\nUrology of Resident Endocrinology of Resident Rheumatology&Immunology of Resident Epidemiology of Resident\\n‘Mmm total mm total ME total ‘Mm total\\nM/AD a KI/RD o\\n° 4 8 # & 8 8B\\nS > gs ea o> ra <2 o> g\\ns & $$ PS Ss & s $s & $$\\n$ e $ $ s ;\\ns of es ” ars & * ° FY & *\\nCardiology of In-charge Respiratory of In-charge Gastroenterology of In-charge Hematology of In-charge\\nME total mm total ° Mm total Ld Mm total\\nfm A /A2/8 mm A /A2/8 a) fm A/A2/B mA /A2/B\\nmm AG/A4 Mm AG/AS Mm AG /A4 AB /A4\\nmm total mm total mm total mm total\\n° mm AI/A2/B = mm A1/A2/B mm AI/A2/B ia mm AV/A2/B\\nmm mm G/M ® mm G/M ® mm A/a\\nmmm Case Analysis | mm Case Analysis Wm Case Analysis mm Case Analysis\\n« 2 © ©\\n° 2 ° 2\\na a 0 °\\n> 2 s s S\\nSE FS SSS s SS es Ss\\nFF SF s os Hr Fr er ow OS\\nor Ss ¢ * eS *\\n Be: SBME (\\n(Question: The definitive diagnostic method for malaria is ( ))\\n‘A.', 'HN G;(e#02i (Bleeding tendency and anemia)\\nC. IFS (Blood routine)\\nD. fl/ BBE!', '(Blood/bone marrow smea)\\nE. iS 28Ba2 (Abdominal ultrasound)\\nJatt ite}358/SHEIA (Why choose this option) ?', 'BE eee iat Fp\\nSHIGE OILIMREOMOD SEE ech rw\\nM/F aie HABE He HR AVIS WTR (This is because\\nblood/bone marrow smears are the “gold standard” for\\nmalaria diagnosis, they can confirm malaria by examining\\nmalaria parasites in the patient’s blood or bone marrow.', 'Other options like radioimmunoassay, medical history, blood\\nroutine, and abdominal ultrasound can also serve as\\nreferences for malaria diagnosis, but they cannot provide\\ndefinitive diagnosis like blood/bone marrow smears) .', 'tea (play et ee ARMAS, ee MDS AHBE.Ox109/L, Ay\\nTbe Ha, SAT(+ ++), ZAC\\nhetesiondh ase -year-old herdsman was admitted for 1 week of fever, with\\nundulant fever.', 'Blood routine test showed leukocytes 6.0x109/L, with a few\\natypical lymphocytes seen, SAT(+ + +).', 'The diagnosis is ( ))\\n‘A.', '{32(Typhoid fever) B. MunE(Sepsis)\\nC. fe (Malaria) D. 4p S875 (Brucellosis)\\nDUHABEX-“NAIA (Why choose this option)?', 'RAMEACIAE, SATS S. (el a cel 2 Bla\\nBATWRAB RE DESROA.', 'HRVRAMOESRE.', 'SD.', 'aL\\nAAEM, ARMS, MRE SMT, Saanaa Nee,\\nFR, BALA, BRST ORAS.', 'Blt, AMARMAINSR (Based\\non the medical history and symptoms, the patient is likely suffering from\\ntyphoid fever.', 'Typhoid fever is an infectious disease caused by Salmonella\\nbacteria, commonly seen in herdsmen and people with poor hygiene habits.', 'The clinical manifestations of typhoid fever include persistent high fever, fatigue,\\ndiarrhea, muscle and joint pain, conjunctival congestion, etc.', 'Blood tests may\\nshow elevated white blood cell count and atypical lymphocytes.', 'Also, the\\npatient has undulant fever, which further supports the diagnosis of typhoid\\nfever.', 'Therefore, option A is the most appropriate answer) ,\\n=m 8 =m 8\\nder different prompts, where (a) and (b) depict the results\\nusing vanilla and Chain-of-Thought prompt, respectively.', 'EE ChalGLWturbo vs. Quen-max\\no 12 3 4 5 67 89\\nDifcuity Level of Questions\\n2 3 4 5 6\\nDiffcuty Level of Questions\\nquestion difficulty, with Level 9 denoting the highest level\\n2022), and MLEC-QA are also useful benchmarks for med-\\nical QA, comprising a large number of high-quality ques-\\ntions, but they were proposed early on, potentially leading\\nto data contamination.', 'Although MedMCQA is a relatively\\nnew benchmark, its questions originate primarily from In-\\ndian medical institutions.', 'As a result, analogous to USMLE,\\nit lacks Chinese medical content and thus cannot serve to\\nevaluate Chinese medical question answering sufficiently\\nwell.', 'CMExam is a novel benchmark for medical QA in Chi-\\nnese, comprising the latest exam questions from CNMLE,\\nwhich has been manually annotated by medical experts with\\nassistance from ChatGPT to ensure high quality.', 'However,\\nCMExam only comprises questions from CNMLE, while\\noverlooking other major medical exams in mainland China.', 'Moreover, as a predominantly multiple choice dataset with\\nonly a small portion of fill-in-the-blank questions, CMExam\\nlacks real-world medical data and scenarios.', 'This constrains\\nits ability to fully assess LLMs’ medical question answering\\ncapabilities.', 'In fact, the optimal evaluation approach is to\\nevaluate LLMs manually (Xu et al.', '2023b).', 'However, man-\\nual evaluation is a time-consuming and labor-intensive task,\\nmaking it difficult to conduct on a large scale.', 'In this paper, we present MedBench, an exhaustive bench-\\nmark specifically designed for the domain of Chinese med-\\nical question answering.', 'Preliminary empirical analyses un-\\nderscore the suboptimal performance of Chinese medical\\nLLMs when subjected to this benchmark, highlighting the\\nneed for improved clinical acumen and diagnostic preci-\\nsion.', 'Furthermore, the adeptness of these LLMs in contex-\\ntual learning requires further refinement.', 'During our empirical investigations, we found that cer-\\ntain models manifest pronounced hallucinatory behavior.', 'As\\nour research progresses, it is imperative to ensure data ve-\\nracity while delving into the systematic evaluation of such\\nhallucinatory phenomena.', 'Furthermore, the appraisal of the\\nLLM’s inferential competencies, as presented in this re-\\nsearch, points to the need for further methodological refine-\\nments.', 'Within the realm of clinical diagnostics, a diagno-\\nsis is typically predicated upon a plethora of corroborative\\nevidence.', 'In subsequent work, we plan to compile an en-\\nriched dataset, encompassing patients’ antecedent medical\\nrecords and comprehensive physical examination narratives,\\nto strengthen the evaluative framework for medical LLMs.', 'Simultaneously, our findings underscore the efficacy of psy-\\nchometric methodologies in aiding the evaluation.', 'Moving\\nforward, we intend to enhance and further integrate such\\nmethodologies into our model assessment paradigm.']\n",
      "184\n",
      "doing sentence : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing sentence : 1\n",
      "doing sentence : 2\n",
      "doing sentence : 3\n",
      "doing sentence : 4\n",
      "doing sentence : 5\n",
      "doing sentence : 6\n",
      "doing sentence : 7\n",
      "doing sentence : 8\n",
      "doing sentence : 9\n",
      "doing sentence : 10\n",
      "doing sentence : 11\n",
      "doing sentence : 12\n",
      "doing sentence : 13\n",
      "doing sentence : 14\n",
      "doing sentence : 15\n",
      "doing sentence : 16\n",
      "doing sentence : 17\n",
      "doing sentence : 18\n",
      "doing sentence : 19\n",
      "doing sentence : 20\n",
      "doing sentence : 21\n",
      "doing sentence : 22\n",
      "doing sentence : 23\n",
      "doing sentence : 24\n",
      "doing sentence : 25\n",
      "doing sentence : 26\n",
      "doing sentence : 27\n",
      "doing sentence : 28\n",
      "doing sentence : 29\n",
      "doing sentence : 30\n",
      "doing sentence : 31\n",
      "doing sentence : 32\n",
      "doing sentence : 33\n",
      "doing sentence : 34\n",
      "doing sentence : 35\n",
      "doing sentence : 36\n",
      "doing sentence : 37\n",
      "doing sentence : 38\n",
      "doing sentence : 39\n",
      "doing sentence : 40\n",
      "doing sentence : 41\n",
      "doing sentence : 42\n",
      "doing sentence : 43\n",
      "doing sentence : 44\n",
      "doing sentence : 45\n",
      "doing sentence : 46\n",
      "doing sentence : 47\n",
      "doing sentence : 48\n",
      "doing sentence : 49\n",
      "doing sentence : 50\n",
      "doing sentence : 51\n",
      "doing sentence : 52\n",
      "doing sentence : 53\n",
      "doing sentence : 54\n",
      "doing sentence : 55\n",
      "doing sentence : 56\n",
      "doing sentence : 57\n",
      "doing sentence : 58\n",
      "doing sentence : 59\n",
      "doing sentence : 60\n",
      "doing sentence : 61\n",
      "doing sentence : 62\n",
      "doing sentence : 63\n",
      "doing sentence : 64\n",
      "doing sentence : 65\n",
      "doing sentence : 66\n",
      "doing sentence : 67\n",
      "doing sentence : 68\n",
      "doing sentence : 69\n",
      "doing sentence : 70\n",
      "doing sentence : 71\n",
      "doing sentence : 72\n",
      "doing sentence : 73\n",
      "doing sentence : 74\n",
      "doing sentence : 75\n",
      "doing sentence : 76\n",
      "doing sentence : 77\n",
      "doing sentence : 78\n",
      "doing sentence : 79\n",
      "doing sentence : 80\n",
      "doing sentence : 81\n",
      "doing sentence : 82\n",
      "doing sentence : 83\n",
      "doing sentence : 84\n",
      "doing sentence : 85\n",
      "doing sentence : 86\n",
      "doing sentence : 87\n",
      "doing sentence : 88\n",
      "doing sentence : 89\n",
      "doing sentence : 90\n",
      "doing sentence : 91\n",
      "doing sentence : 92\n",
      "doing sentence : 93\n",
      "doing sentence : 94\n",
      "doing sentence : 95\n",
      "doing sentence : 96\n",
      "doing sentence : 97\n",
      "doing sentence : 98\n",
      "doing sentence : 99\n",
      "doing sentence : 100\n",
      "doing sentence : 101\n",
      "doing sentence : 102\n",
      "doing sentence : 103\n",
      "doing sentence : 104\n",
      "doing sentence : 105\n",
      "doing sentence : 106\n",
      "doing sentence : 107\n",
      "doing sentence : 108\n",
      "doing sentence : 109\n",
      "doing sentence : 110\n",
      "doing sentence : 111\n",
      "doing sentence : 112\n",
      "doing sentence : 113\n",
      "doing sentence : 114\n",
      "doing sentence : 115\n",
      "doing sentence : 116\n",
      "doing sentence : 117\n",
      "doing sentence : 118\n",
      "doing sentence : 119\n",
      "doing sentence : 120\n",
      "doing sentence : 121\n",
      "doing sentence : 122\n",
      "doing sentence : 123\n",
      "doing sentence : 124\n",
      "doing sentence : 125\n",
      "doing sentence : 126\n",
      "doing sentence : 127\n",
      "doing sentence : 128\n",
      "doing sentence : 129\n",
      "doing sentence : 130\n",
      "doing sentence : 131\n",
      "doing sentence : 132\n",
      "doing sentence : 133\n",
      "doing sentence : 134\n",
      "doing sentence : 135\n",
      "doing sentence : 136\n",
      "doing sentence : 137\n",
      "doing sentence : 138\n",
      "doing sentence : 139\n",
      "doing sentence : 140\n",
      "doing sentence : 141\n",
      "doing sentence : 142\n",
      "doing sentence : 143\n",
      "doing sentence : 144\n",
      "doing sentence : 145\n",
      "doing sentence : 146\n",
      "doing sentence : 147\n",
      "doing sentence : 148\n",
      "doing sentence : 149\n",
      "doing sentence : 150\n",
      "doing sentence : 151\n",
      "doing sentence : 152\n",
      "doing sentence : 153\n",
      "doing sentence : 154\n",
      "doing sentence : 155\n",
      "doing sentence : 156\n",
      "doing sentence : 157\n",
      "doing sentence : 158\n",
      "doing sentence : 159\n",
      "doing sentence : 160\n",
      "doing sentence : 161\n",
      "doing sentence : 162\n",
      "doing sentence : 163\n",
      "doing sentence : 164\n",
      "doing sentence : 165\n",
      "doing sentence : 166\n",
      "doing sentence : 167\n",
      "doing sentence : 168\n",
      "doing sentence : 169\n",
      "doing sentence : 170\n",
      "doing sentence : 171\n",
      "doing sentence : 172\n",
      "doing sentence : 173\n",
      "doing sentence : 174\n",
      "doing sentence : 175\n",
      "doing sentence : 176\n",
      "doing sentence : 177\n",
      "doing sentence : 178\n",
      "doing sentence : 179\n",
      "doing sentence : 180\n",
      "doing sentence : 181\n",
      "doing sentence : 182\n",
      "all calls done\n",
      "Got obtained data from BERN2\n",
      "Parsed Everything\n",
      "all stuff parsed\n",
      "Created the base article and linked the authors\n",
      "Pushed all data to graph !\n"
     ]
    }
   ],
   "source": [
    "authors =[\"Yan Cai\", \"Linlin Wang\", \"Ye Wang\", \"Gerard de Melo\", \"Ya Zhang\", \"Yanfeng Wang\", \"Liang He\"]\n",
    "title = \"MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models\"\n",
    "threshold = \"Introduction\"\n",
    "numInterestingPage = 7\n",
    "url = \"https://arxiv.org/pdf/2312.12806.pdf\"\n",
    "\n",
    "parsedstuff = getParsedEntity(url, numInterestingPage, threshold)\n",
    "print(\"all stuff parsed\")\n",
    "pushtoGraph(parsedstuff, authors, title)\n",
    "relations = getRelations(parsedstuff)\n",
    "pushRelationToGraph(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c120d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libchebipy._chebi_entity import ChebiEntity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e2492c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A homopolymer, composed of poly(caprolactone) macromolecules.\n",
      "A homopolymer, composed of poly(caprolactone) macromolecules.\n",
      "A polymer composed of repeating hydroxyacetic acid units.\n",
      "A polymer composed of repeating (S)-2-hydroxypropanoyl units.\n",
      "A macromolecule composed of repeating 2-hydroxypropanoyl units.\n",
      "A dipeptide formed from L-tyrosine and L-alanine residues.\n",
      "A dipeptide formed from L-tyrosine and L-glutamic acid residues.\n",
      "A homopolymer, composed of omega-hydroxypoly(furan-2,5-diylmethylene) macromolecules.\n",
      "A diamino acid that is caproic (hexanoic) acid bearing two amino substituents at positions 2 and 6.\n"
     ]
    }
   ],
   "source": [
    "result = neo4j_query(\"\"\"\n",
    "MATCH (e:Entity)\n",
    "WHERE ANY(id IN e.other_ids WHERE id CONTAINS \"CHEBI\")\n",
    "WITH e,\n",
    "  [id in e.other_ids WHERE id CONTAINS \"CHEBI\" | split(id, \":\")[1]][0] as chebiId\n",
    "RETURN e.name, chebiId\n",
    "\"\"\")\n",
    "\n",
    "chebiEntity_list = []\n",
    "\n",
    "for index, row in result.iterrows():\n",
    "    chebiID = row['chebiId']\n",
    "    \n",
    "    try:\n",
    "        # Attempt to create ChebiEntity\n",
    "        entity = ChebiEntity(chebiID)\n",
    "        chebiEntity_list.append(entity)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "chebEntityFiltered = [entity for entity in chebiEntity_list if entity.get_definition() is not None]\n",
    "        \n",
    "for entity in chebEntityFiltered:\n",
    "    print(entity.get_definition())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fcc51b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'chebiId': 'CHEBI:60736', 'chebiDefinition': 'A homopolymer, composed of poly(caprolactone) macromolecules.'}, {'chebiId': 'CHEBI:60736', 'chebiDefinition': 'A homopolymer, composed of poly(caprolactone) macromolecules.'}, {'chebiId': 'CHEBI:53492', 'chebiDefinition': 'A polymer composed of repeating hydroxyacetic acid units.'}, {'chebiId': 'CHEBI:53408', 'chebiDefinition': 'A polymer composed of repeating (S)-2-hydroxypropanoyl units.'}, {'chebiId': 'CHEBI:53407', 'chebiDefinition': 'A macromolecule composed of repeating 2-hydroxypropanoyl units.'}, {'chebiId': 'CHEBI:74879', 'chebiDefinition': 'A dipeptide formed from L-tyrosine and L-alanine residues.'}, {'chebiId': 'CHEBI:74883', 'chebiDefinition': 'A dipeptide formed from L-tyrosine and L-glutamic acid residues.'}, {'chebiId': 'CHEBI:60594', 'chebiDefinition': 'A homopolymer, composed of omega-hydroxypoly(furan-2,5-diylmethylene) macromolecules.'}, {'chebiId': 'CHEBI:25094', 'chebiDefinition': 'A diamino acid that is caproic (hexanoic) acid bearing two amino substituents at positions 2 and 6.'}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_for_update = [{'chebiId': entity.get_id(), 'chebiDefinition': entity.get_definition()} for entity in chebEntityFiltered]\n",
    "print(entities_for_update)\n",
    "# Pass the list of dictionaries as a parameter to the Neo4j query\n",
    "neo4j_query(\"\"\"\n",
    "UNWIND $entities as entity\n",
    "MATCH (e:Entity)\n",
    "WHERE ANY(id IN e.other_ids WHERE id = entity.chebiId)\n",
    "SET e.chebiDefinition = entity.chebiDefinition\n",
    "\"\"\", {'entities': entities_for_update})"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
